# CaiCache
## 基于Go的分布式缓存框架实现

### 架构世界，缓存为王
分布式缓存解决了什么样的问题？

**缓存**：对于某种耗时操作的频繁请求，将该操作的结果暂存，再次遇到改请求，则直接返回相应结果。

缓存的引入可以快速返回请求结果，但也会带来其他的问题。

缓存一般保存在内存，内存不够了怎么办？-> 合理的淘汰策略

缓存结果的写入和读取一般是并行的，并发读写的冲突如何解决？-> 加锁

单机性能不够如何解决？-> 水平扩展：分布式缓存，垂直扩展：增加单节点性能

CaiCache，在考虑资源控制、淘汰策略、并发、分布式节点通信等各个方面的问题下的分布式缓存系统实现。

参考：memcached, groupcache

支持特性：
- 单机缓存和基于HTTP通信的分布式缓存
- 最近最少访问LRU(Least Recently Used)缓存策略
- 基于Go锁机制的缓存击穿解决方案
- 基于一致性哈性的负载均衡实现
- 基于Protobuf的节点通信
- ...

前置知识点：
- Go语言语法基础
- Go test单元测试基础
- Go Protobuf基础
- Go Web服务基础

### LRU缓存淘汰策略
为提高读取效率，缓存全部存在内存，但内存有限，无法无限制增加数据，必须有合理的淘汰策略移除无用的数据。

补充：三种常用的淘汰策略，FIFO，LFU，LRU
- FIFO: 先进先出，越早添加越先淘汰
  - 实现基于队列，新增记录添加到队尾，每次内存不够时，淘汰队首。
  - 部分数据最早添加，也最长被访问，但基于FIFO会出现频繁的添加，淘汰现象，降低缓存命中率。
- LFU: 最少使用，淘汰缓存中访问频率最低的记录
  - 实现基于一个按照访问次数排序的队列，每次访问，访问次数加1，队列重新排序，淘汰访问次数最少的即可。
  - 维护记录的访问次数，对内存消耗较高；受到历史数据的影响较大。
- LRU: 最近最少使用，如果数据最近被访问过，那么将来被访问的概率也会更高
  - 实现基于维护一条队列，如果某条记录被访问了，则移动到队尾，每次淘汰队首元素即可。

#### LRU算法实现
Cache 数据结构：存储键-值映射关系的map，基于双向链表的队列。
基于Cache的 `New(), Get(), RemoveOldest(), Add()` 方法。
参考代码 `/lru/lru.go`